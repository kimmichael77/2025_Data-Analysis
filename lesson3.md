## 2. 회귀분석 : 연관문제

### 2.1. 변수선택법

$n \ll p$ 문제를 해결하기 위한 가장 간단하고 직관적인 방법은 변수를 버리는 것이다. 그렇다면 버리는 변수를 어떻게 고를 것인가? 데이터 사이언스, 머신러닝 분야에서 변수를 선택(feature selection)하는 처리방법을 크게 필터(filter), 래퍼(wrapper), 임베디드(embedded) 방법으로 구분한다. (통계학에서는 이러한 방식의 구분에 크게 의미를 두지 않는다.)

필터 방법이란, 분석 방법과 무관하게 데이터 자체가 가진 고유의 속성을 이용하여 변수를 선택하는 방법이다. 즉, 변수 각각에 대해 통계적인 척도를 정해서 임계치를 넘어가는 변수만을 선택하는 방법이다. C4.5 알고리즘(엔트로피를 이용하여 의사결정나무를 성장하는 알고리즘)이 대표적인 예이며, 그 이외에도 변수 선택을 위해 분산, 카이제곱 통계량, 상관계수 등을 활용하기도 한다.

래퍼 방법이란, 필터 방법과 정반대 패러다임을 가지고 분석 방법에 최적인 변수를 선택하는 방법이다. 분석 모형을 기준으로 평가했을 때에 가장 불필요한 변수를 버리거나 가장 필요한 변수를 추가하는 방법으로 변수를 선택한다. 아래에서 소개할 변수선택법은 래퍼 방법의 범주에 해당한다.

임베디드 방법이란, 분석 방법에 변수를 선택하는 메커니즘이 내재되어있는 방법이다. 정규화 선형회귀 중 라쏘(LASSO) 회귀가 임베디드 방법의 대표적인 예이다.  

아래에서는 통계학에서 지칭하는 변수 선택(subset selection) 방법인 전진선택법, 후진소거법, 단계적 선택법을 설명한다.


#### 2.1.1. 변수선택의 알고리즘

세부 변수선택 방법의 기본적인 아이디어는 어떤 책을 봐도 동일하지만, 알고리즘의 세부 전개방법이나 명칭은 서적마다 차이가 있다. 아래의 설명은 ISL을 기준으로 알고리즘을 기술하였고, 일반적인 국내의 통계서적에서 설명하는 변수선택방법을 부수적으로 추가하여 기술하였다. 

##### 2.1.1.1. 최량변수선택법(best subset selection)

전체 변수로 만들 수 있는 모든 유형의 변수조합을 만든 후 평가기준이 되는 통계량([2.1.2. 참조](#212-변수선택의-기준))을 바탕으로 가장 좋은 변수조합을 선택하는 방법. 이론적으로 구상이 가능한 모든 변수조합을 검토할 수 있다는 점에서 장점이 있지만 변수의 수에 따라 연산량이 기하급수적으로 늘어난다는 단점이 있다. 변수의 수가 p개인 경우 구상이 가능한 변수조합의 수는 아래와 같다.

```math
\sum_{i = 0}^p \binom p i = 2^p
```

ISL에서는 이론적으로 가장 간단하고 직관적인 변수선택 방법으로 소개하고 있으나 일반적인 통계학 서적에서는 변수선택법으로 제시하고 있지 않는다. 하지만, 전진선택, 후진소거, 단계적선택법이 공통적으로 가지고 있는 맹점이 최적의 변수조합을 보장하지 못한다는 점과 컴퓨팅 비용의 감소로 인해 연산 가능한 모형의 개수가 크게 늘어났다는 점을 고려했을 때에 의외로 응용할 여지가 많은 방법이다.

##### 2.1.1.2. 전진선택법(forward stepwise selection)

절편만 있는 영모형(null model)에서 시작해서 단계별로 변수를 하나씩 추가할지 여부를 결정하고 변수를 추가하지 않아도 되는 시점이 오면 모형을 확정하는 방법이다. 구체적인 알고리즘의 순서는 다음과 같다.

- $\mathcal M_0 = $ null model

- $\mathcal M_k$, $k = 0, 1, \cdots, p-1$에 대해

    - 1단계 : $\mathcal M_k$에 변수 1개를 추가한 $p - k$개의 모형을 만든다.

    - 2단계 : $p - k$개 모형과 $\mathcal M_k$의 평가기준 통계량을 계산하여 가장 우수한 모델이 $\mathcal M_{k+1}$이 된다.

    - 3단계 : $\mathcal M_{k} = \mathcal M_{k+1}$이면 알고리즘을 종료

    - 4단계 : 1단계로 돌아간다.

최량변수선택법과 비교했을 때에 전진선택법은 연산량이 극적으로 감소한다는 장점이 있다. 전진선택법을 통해 선택한 최종모형이 포화모형(saturated model / full model)이 되는 경우의 연산량은 다음과 같다.

```math
1 + \frac{ p(p+1) }{2}
```

$p = 20$인 경우 최량변수선택법은 100만개 이상의 모형을 비교해야 하지만 전진선택법은 최종단계에 이르기까지 211개의 모형만 비교하면 된다. 하지만 한 번 선택한 변수를 버리는 알고리즘이 없기 때문에 최적의 모형을 보장하지 못한다.  

ISL은 forward stepwise라고 소개하고 있지만 일반적으로 전진선택법은 forward selection이라고 표기한다.

##### 2.1.1.3. 후진소거법(backward stepwise selection)

후진소거법은 전진선택법과 반대로 포화모형에서 시작하여 변수를 하나씩 제거하는 방법이다. 구체적인 알고리즘은 다음과 같다.

- $\mathcal M_p = $ saturated model

- $\mathcal M_k$, $k = p, p-1, \cdots, 1$에 대해

    - 1단계 : $\mathcal M_k$에서 변수 1개를 뺀 $k - 1$개의 모형을 만든다.

    - 2단계 : $p - k$개 모형과 $\mathcal M_k$의 평가기준 통계량을 계산하여 가장 우수한 모델이 $\mathcal M_{k-1}$이 된다.

    - 3단계 : $\mathcal M_{k} = \mathcal M_{k-1}$이면 알고리즘을 종료

    - 4단계 : 1단계로 돌아간다.

후진소거법도 전진선택법과 동일한 장단점을 가진다. 

ISL의 표기와 달리 일반적으로 backward elemination이라고 표기한다. R에서 step() 함수를 이용할 때에 method 인자를 별도로 지정하지 않으면 후진소거법을 이용하여 변수를 선택한다.

##### 2.1.1.4. 단계적선택법(hybrid stepwise selection)

전진선택법과 후진소거법이 가지는 단점을 보완하기 위하여 각 단계에서 변수추가와 변수삭제를 동시에 고려하여 모형을 확장하는 방법이다. 구체적인 알고리즘은 다음과 같다. 

- $\mathcal M_0 = $ null model

- $\mathcal M_k$, $k = 0, 1, \cdots, p-1$에 대해

    - 1단계 : $\mathcal M_k$에 변수 1개를 추가한 $p - k$개의 모형을 만든다.

    - 2단계 : $p - k$개 모형과 $\mathcal M_k$의 평가기준 통계량을 계산하여 가장 우수한 모델이 $\mathcal M_{k+1}$이 된다.

    - 3단계 : $\mathcal M_{k+1}$에서 변수 1개를 뺀 모형과 $\mathcal M_{k+1}$의 평가기준 통계량을 계산하여 가장 우수한 모델이 $\mathcal M_{k+1}$이 된다.

    - 4단계 : $\mathcal M_{k} = \mathcal M_{k+1}$이면 알고리즘을 종료

    - 5단계 : 1단계로 돌아간다.

단계적 선택법의 연산량은 전진/후진 방법보다 2배까지 많아질 수 있지만, 한 번 선택한 변수를 제거하는 옵션을 가지고 있기 때문에 전진/후진 방법이 가지는 단점을 일부 보완할 수 있다. 하지만 단계적 선택법 역시 경로의존적이기 때문에 최적의 조합을 보장하지는 않는다.

---

#### 2.1.2. 변수선택의 기준

변수선택의 기준이 되는 통계량은 주로 AIC를 활용하고 BIC, $C_p$ 정도를 언급하지만, 서적에 따라 수정된 결정계수와 유의확률을 이용하는 경우도 있다. 기준통계량이 가지는 공통적인 특성은 모형의 성능을 평가하는 손실함수항과 변수가 많아질 때의 벌점항으로 구성되어있다는 점이다. (각 통계량의 이론적인 배경 설명은 생략)

##### 2.1.2.1. 아카이케 정보기준(AIC)

최대우도법에 이론적 기반을 둔 손실함수(잔차제곱합)를 정의한 후, 변수의 개수에 상수를 곱하는 벌점항을 더한 통계량. 작을수록 좋은 모형.

```math
AIC = -2\ln(\hat L) + 2p
```

이때 변수의 수는 p이다.


##### 2.1.2.2. 베이즈 정보기준(BIC)

베이지안 이론에 기반을 두고 사후확률이 높은 모형을 기준으로 손실함수와 벌점항을 정의. 작을수록 좋은 모형.

```math
BIC = -2 \ln(\hat L) + \ln(n)p
```

이때 변수의 수는 p, 관측치의 수는 n이다. $\ln(1) > 2$이므로 동일한 조건 하에서 BIC의 벌점항은 AIC보다 p에 민감하게 반응하고 관측치가 많으면 많을수록 벌점항의 가중치가 커진다. 이 때문에 BIC는 AIC에 비해 변수의 수를 억제하는 경향성이 강하다.

##### 2.1.2.3. Mallow's $C_p$

예측오차 최소화를 이론적 배경으로 하는 통계량. 통계량이 선택한 변수의 수와 유사할수록 좋은 모형.

```math
C_p = \frac {SSE}{\hat \sigma^2} - (n - 2p)
```

##### 2.1.2.3. 수정된 결정계수($R^2$)

변수의 수를 역가중하여 계산하는 수정된 결정계수를 변수선택의 기준으로 삼는 경우도 있다.

```math
R_{adj}^2 = 1 - \frac{(n-1)(1-r^2)}{n-p-1}
```

##### 2.1.2.4. 유의확률(p-value)

아주 드물지만 모형의 적합도검정 유의확률을 기준으로 변수선택을 설명한 교재도 있다. 이 경우 유의확률이 낮은 모형을 우선 선택한다.

---

#### 2.1.3. 예제해설

---

### 2.2. 정규화 선형회귀

#### 2.2.1. 원리

$n \ll p$인 경우 모수추정치의 분산이 매우 커지는 문제를 2차시에 확인했다. 정규화 선형회귀는 분산과다로 인해 모수추정치의 정확성이 떨어지는 문제를 해결하기 위해 편향을 주어 분산을 축소하는 방식으로 회귀계수를 조정하는 형태의 회귀분석방법이다. '분산 + 편향'으로 이루어지는 모형의 손실함수(선형회귀의 경우 RSS)에서 LSE를 이용하여 계산한 불편추정치인 회귀계수에 약간의 편향을 주면서 큰 분산의 감소를 얻을 수 있으면 모형의 손실함수가 전체적으로 감소할 수 있다는 점에 착안하였다. 회귀계수에 규제(regularization)를 주었기 때문에 regularized regression이라고 부른다.

##### 2.2.1.1. 목적함수와 제약조건

정규화 선형회귀의 모형은 목적함수와 제약조건으로 구성된다. 능형(릿지) 회귀의 목적함수와 제약조건은 다음과 같으며, 제약조건 하에서 목적함수를 최소화하는 $\boldsymbol \beta$가 해가 된다. 라쏘 회귀와 엘라스틱넷 역시 기본적인 구조는 같고 제약조건에 차이가 있다.

```math
\boxed{\argmin_\beta\sum_{i = 1}^n \bigg ( y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \bigg)^2}_\text{목적함수 = RSS 최소화} \\

s.t. \boxed{\sum_{j = 1}^p \beta_j^2 \le t}_\text{제약조건 = 벌점항 상한}
```

##### 2.2.1.2. 해를 구하는 방법 : 라그랑주 승수법

목적함수 + 제약조건이 주어진 문제의 해를 풀이하는 일반적인 방법이 라그랑주 승수법이다. 라그랑주 승수법은 목적함수 + 제약조건으로 이루어진 라그랑지안 함수를 정의한 후 구하고자하는 변수로 1차미분하거나 알고리즘을 이용하여 수치적으로 해를 추정하는 방법으로 구한다.

---

#### 2.2.2. 제약조건에 따른 분류

##### 2.2.2.1. 능형회귀(Ridge Regression)

능형회귀는 정규화선형회귀 중에서 제약조건을 $||\beta||_2^2$ 차원에서 정의한다. 위에서 기술한 목적함수와 제약조건에 따른 능형회귀의 라그랑지안 함수는 다음과 같다.

```math
\argmin_\beta \bigg\{ \sum_{i = 1}^n \big ( y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \big)^2 + \lambda \sum_{j = 1}^p \beta_j^2 \bigg\}
```

벌점항은 각 회귀계수의 제곱을 더한 값에 $\lambda$를 곱한 값이다. 특정 회귀계수가 매우 크면 벌점항의 크기가 커지기 때문에 제약조건을 따르기 위해 회귀계수를 작게 만들어야 하는데, 이 과정에서 $\beta_j x_{ij}$값의 감소가 목적함수의 증가로 이어지는 회귀계수는 회귀계수 감소의 폭이 작아지고 목적함수의 증가 폭이 작은 회귀계수는 감소폭이 커진다. 즉, 목적함수와 제약조건 사이에서 적절한 균형을 이루어 최종적으로 <편향 + 분산>의 합산값이 작아지는 회귀계수 조정이 이루어지는 것이다.

이때, $\lambda$는 초모수(hyperparameter)로 벌점항과 손실함수 사이의 밸런스를 어떻게 조정할 것이냐를 결정한다. $\lambda$가 커지면 회귀계수의 미세한 증가에도 벌점항이 커져 목적함수가 증가하기 때문에 회귀계수가 큰폭으로 줄어든다. 이론상으로는 분석자가 값을 정해야 하나 실제로는 회귀모형을 적합할 때에 $\lambda$값의 차이에 따른 회귀계수의 차이를 가시적으로 확인하고 적절한 $\lambda$를 선택한다.

능형회귀는 $\beta^2$과 RSS를 비교하여 회귀계수 조정이 이루어지는데, 변수의 단위가 다른 경우 왜곡이 생길 수 있다. 따라서, 능형회귀를 수행할 때에는 반드시 각 변수들을 표준화해야한다.


##### 2.2.2.2. 라쏘회귀(LASSO Regression, Least Absolute Shrinkage and Selection Operation)

라쏘회귀는 제약조건을 $||\beta||_1$ 차원에서 정의한다. 

```math
\argmin_\beta \bigg\{ \sum_{i = 1}^n \big ( y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \big)^2 + \lambda \sum_{j = 1}^p | \beta_j | \bigg\}
```

벌점항은 각 회귀계수의 절대값을 더한 값에 $\lambda$를 곱한 값이다. 목적함수와 제약조건의 조정이 이루어지는 과정은 능형회귀와 유사하게 이해할 수 있으나 두 가지 차이가 있다. 첫째로, 능형회귀는 미분이 가능하기 때문에 대수적으로 해를 구할 수 있다. (참고로 능형회귀의 해는 $\hat \beta_{ridge} = (\boldsymbol{X'X} + \lambda \boldsymbol I)^{-1}\boldsymbol {X'y}$이다.) 반대로 라쏘회귀의 제약조건은 $\beta = 0$인 지점에서 미분이 불가능한 함수이기 때문에 대수적으로는 해를 구할 수 없다. 둘째로, 라쏘회귀의 해를 구하기 위해 일정한 알고리즘을 반복하는데, 이 과정에서 제약조건 > 0 규칙을 두고 $\beta$값의 범위를 조정한다. 즉, 알고리즘 반복 과정에서 $\beta$가 특정 영역 안에 들어가면 0으로 값을 조정하는 단계가 포함이 되어있기 때문에 $\lambda$값에 따라 $\beta = 0$인 구간이 존재한다. 

두 번째 특성으로 인해 라쏘회귀는 자동으로 변수를 선택할 수 있는 분석기법으로 분류되기도 한다. (임베디드 방법) 반면, 라쏘회귀를 통해 변수를 제거하는 순서가 변수의 중요도와 관련이 있는지에 대해서는 논란이 있다. 즉, 라쏘회귀를 통해 제거하는 변수가 남아있는 변수보다 반응변수 설명에 더 유용한 변수일 수도 있다는 의미이다.

##### 2.2.2.3. 엘라스틱넷

엘라스틱넷은 제약조건을 $||\beta||_1$와 $||\beta||_2^2$ 차원을 혼합하여 정의한다.

```math
\argmin_\beta \bigg\{ \sum_{i = 1}^n \big ( y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \big)^2 + \lambda \sum_{j = 1}^p \big(\alpha| \beta_j | + (1 - \alpha)\beta_j^2 \big) \bigg\}
```

엘라스틱넷은 능형/라쏘의 $\lambda$외에 $\alpha$라는 초모수를 하나 더 가진다. 이는 절대값을 기준으로 계산한 벌점과 제곱을 기준으로 계산한 벌점의 혼합비율이다. $0 < \alpha < 1$의 범위에서 초모수가 결정되며, $\alpha = 1$이면 라쏘, $\alpha = 0$이면 능형회귀와 같아진다.

이러한 엘라스틱넷의 구조는 능형회귀와 라쏘회귀의 장점을 결합하기 위해 설계된 것이다. 능형회귀는 다중공선성을 가진 데이터를 안정적으로 처리하는데에 강점이 있는 반면 중요도가 떨어지는 변수를 제거할 수 없다. 라쏘회귀는 변수 제거가 가능하지만 변수간 상관관계가 높은 경우에 변수 선택 순서가 매우 불안정한 것으로 알려져있다. 엘라스틱넷은 둘 사이의 장점을 결합하여 안정적으로 상관관계가 있는 변수 간의 회귀계수를 조정하면서 일정 범위에서 중요도가 떨어지면 변수를 제거할 수 있는 분석 방법으로 알려져있다.

---

#### 2.2.3. $n \ll p$ 데이터 처리와 정규화 선형회귀

정규화 선형회귀는 설명변수 간 상관관계가 있어서 회귀모수 추정치가 불안정한 문제를 해결하기 위한 목적으로 고안되었다. 이론적으로 선형회귀계수는 불편추정량이라는 점이 증명되어있는데, 상관관계가 있는 설명변수로 인해 회귀계수의 분산이 지나치게 높아지는 점을 불편성을 포기함으로써 모형 전체의 잔차를 줄일 수 있다는 점에 착안한 것이다. 이 때문에 정규화 선형회귀는 데이터의 양적 증가나 특성 때문에 발생하는 $n \ll p$ 문제를 직접적으로 해결할 수 있는 방법은 아니다. 즉, $n \ll p$ 문제가 낳는 선형회귀모형 해의 부존재 문제를 근본적으로 해결하지 못한다.

하지만 선형회귀가 가지는 최대 강점인 해석력(interpretability)을 포기하지 않으면서 $n \ll p$ 문제가 있는 데이터에 모형을 적합했을 때에 발생할 수 있는 문제를 어느 정도는 완화할 수 있다는 장점을 가지고 있다. 특히, 라쏘회귀의 경우 초모수의 범위에 따라서 일정범위 내에서 변수를 제외하는 효과가 있기 때문에 차원축소와 변수선택의 관점에서도 유용한 대안으로 활용할 수 있다.

반면, 분석 방법이 가지는 한계를 잘 인지하고 활용하는 것이 중요하다. 우선, 선형회귀모형이 $n \ll p$ 문제에 제대로 대응하지 못하는 것은 데이터 자체의 한계에서 기인하는 측면이 있다. 데이터가 가지고 있는 다차원의 정보관계를 유의미하게 구분하기 위해서는 충분히 많은 n이 확보되어야 하지만 그렇지 못하기 때문에 생기는 분석과 해석의 어려움이 있는데, 이 부분은 정규화 선형회귀뿐만 아니라 이후에 학습할 모든 유형의 분석방법을 다룰 때에 똑같이 잔존하는 문제이다. 어떤 분석 방법을 활용하더라도 데이터 자체가 가지고 있는 한계점을 완벽하게 극복하는 것은 어렵기 때문에 분석의 목적을 달성하는 것을 방해하는 문제를 국소적으로 해결하는 방법 중 하나로써 정규화 선형회귀가 가치를 가지는 것이라는 의미이다. 

분석방법의 본질 측면에서 정규화 선형회귀는 해석력 보존과 선형모형 유지라는 목적을 위해서 활용하는 분석방법이다. $n \ll p$인 데이터를 가지고 있을 때에 이론 측면에서 변수 간의 인과관계를 최대한 잘 설명할 수 있는 회귀모형을 고를 때에 선택할 수 있는 분석방법이라는 의미이다. 그런데 데이터 분석, 머신러닝 과정에서 정규화 선형회귀, 특히 라쏘회귀를 레퍼런스 모델로 삼거나 후보모델 중 하나로 두고 비교하는 사례를 공모전 등에서 심심치않게 볼 수 있다. 이는 정규화 선형회귀가 가지고 있는 내재적인 의미와 가치를 전혀 이해하지 못하고 기계적으로 분석 방법을 적용하는 예라고 볼 수 있다. 회귀분석의 목적은 (1) 데이터 설명, (2) 모수 추정, (3) 예측, (4) 변수 통제 등으로 구분할 수 있는데(Introduction to Linear Regression Analysis, D. Montgomery et. al., 2012, p. 9) 실제 데이터를 이용하는 경우 회귀모형의 예측능력은 활용성이 거의 없다고 할 정도로 떨어진다. 선형회귀의 본질을 보존하는 상태에서 모수에 약간의 수정을 가하는 정규화 선형회귀 역시 회귀모형의 이와 같은 특성을 그대로 공유하기 때문에 예측에 최우선 관심이 있는 데이터 분석에서 정규화 선형회귀를 활용하는 것은 분석의 목적에 전혀 맞지 않는 분석이라고 할 수 있다. 

따라서, 정규화 선형회귀는 $n \ll p$ 문제가 있는 상황에서 해석력에서 최대한 양보하지 않기 위해서 선택하는 보완책 정도로 이해하는 것이 좋다. (애초에 '선형성'을 전제로 하기 때문에 데이터 분석에서 생기는 많은 장점과 단점이 병존하고 있고, 어떤 점이 장점이고 어떤 점이 단점인지를 분명히 인식한 후 선형회귀모형을 선택하는 것이 좋습니다.)

---

#### 2.2.4. 예제 해설


---

### 2.3. 일반화 가법모형(General Additive Model, GAM)

일반화 가법모형(GAM)은 GLM에서 반응변수 기댓값을 연결함수로 변환하여 모형을 확장시킨 것과 비슷한 방법으로 설명변수를 함수변환하여 반응변수와 관계를 정의한 모형이다. 변수와 모형의 관계를 일반화시켜 설명변수와 반응변수 간의 관계가 선형이 아닌 경우까지를 반영하여 모형을 설정할 수 있게 한 것이 GAM의 목적이다. 

#### 2.3.1. 모형

GAM의 개념적 모형은 아래와 같다.

```math
g(\mu_i) = \beta_0 + f_1(x_{1i}) + f_2(x_{2i}) + \cdots + f_p(x_{pi})
```

$g(\cdot)$는 반응변수의 연결함수이고, $f_j(x_{ji})$는 j번째 설명변수에 대한 평활함수를 의미한다. 

위 모형에 따르면 모든 반응변수는 각각의 방법에 따라 예측변수와 연결되는 독자적인 함수관계를 가지고, 이러한 반응변수들의 합으로 예측변수의 평균 변환값이 구성된다. 이는 예측변수와 반응변수의 비선형관계를 담을 수 있는 평활함수를 도입하여 선형회귀모형이 가지는 선형성의 한계를 극복할 수 있게 한다. 동시에 각 설명변수의 평활화한 값은 합으로 연결된다. 이는 회귀분석이 가지는 강점인 해석가능성을 살리기 위해 설명변수와 반응변수의 관계를 각각의 변수별로 독립적으로 구성하기 위함이다. 즉, 첫 번째 설명변수가 반응연수와 양의 2차함수 관계를 가지고 두 번째 설명변수가 반응변수와 음의 2차함수 관계를 가지는 경우, 합으로 연결되면 각 설명변수의 변화에 따른 반응변수의 변화를 해석할 수 있도록 구성한 것이다.

이러한 특성 때문에 GAM을 블랙박스보다 해석 측면에서 우월하다는 의미로 회색 상자(gray box)라고 부르기도 한다.

##### 2.3.1.1 목적함수와 제약조건

가법모형의 해를 구하는 과정은 데이터를 잘 설명할 수 있는 평활함수를 찾아내는 과정이다. 이는 다음과 같은 최적화식으로 표현할 수 있다.

```math
\argmin \sum_{i = 1}^n \bigg(y_i - \beta_0 - \sum_{j = 1}^p f_j(x_{ji}) \bigg)^2 \\
s.t. \sum_{j = 1}^p \lambda \int f''_j (t_j)^2 dt_j
```

위 식을 해석하면, RSS와 유사한 형태로 정의할 수 있는 손실함수(실제 반응변수값과 모형에서 예측하는 반응변수값 간의 차이의 제곱합)를 최소화하는 $f(\cdot)$를 찾는데, 평활함수의 $t_j$점에서 2계 도함수값의 제곱적분값에 평활화 초모수(매개변수) $\lambda$를 곱한 값의 총합이 일정 기준치보다 낮은 범위에서 $f(\cdot)$를 찾는 것으로 이해할 수 있다.

벌점항은 $f$ 함수가 결합되는 지점에서 연속성과 기울기의 방향 및 정도가 모두 보장되도록 3차 스플라인 조건을 기반으로 구성하였다. (수업시간에 좀 더 자세히 설명하겠습니다.)

위 문제의 해는 대수적으로 풀 수 없기 때문에 알고리즘을 이용하여 해를 푸는 것이 일반적인 방법이다. 이때의 알고리즘은 반응변수가 가지는 분포의 종류에 따라 달라지는데, 가우시안 분포의 경우 역적합 알고리즘, 로지스틱이나 포아송 분포의 경우 로컬 스코어링 역적합 알고리즘을 사용한다. 좀 더 일반적으로 지수족 분포를 따르는 경우에는 벌점 회귀스플라인 알고리즘을 많이 사용한다. 

##### 2.3.1.2. 역적합 알고리즘(Backfitting Algorithm)

수학적인 엄밀성과 해의 조건 측면에서 알고리즘마다 차이는 있으나 알고리즘의 전개 과정은 대동소이하기 때문에 가장 기초적인 역적합 알고리즘을 기준으로 해를 구하는 과정을 소개한다. 

- 초기화 : $\hat \beta_0 = \frac 1 n \sum_i^n y_i$, $\hat f_j \equiv 0, \forall i, j$

- while $\varDelta \hat f_j > t$

    - $\hat f_j := S_j \bigg[  | y_i - \hat \alpha - \sum_{k \neq j} \hat f_k(x_{ik}) |_1^n \bigg]$

    - $\hat f_j := \hat f_j - \frac 1 n \sum_{i = 1}^n \hat f_j(x_{ij}) $

위 알고리즘의 의미를 해석하면 다음과 같다.

절편은 반응변수의 평균값으로 정하고 알고리즘을 거치면서 이 값은 변하지 않는다.

최초의 평활함수 $\hat f$는 영함수이다. $S_j$는 스플라인 함수로, 각 관측치마다 반응변수에서 현재까지 설정되어있는 절편과 평활함수값의 합을 뺀 잔차($y_i - \hat \alpha - \sum_{k \neq j} \hat f_k(x_{ik})$)를 평활화한 값을 새로운 평활함수로 갱신한다. 다음으로 평활함수값의 평균치를 중심으로 평활함수를 재중심화한 값을 새로운 평활함수값으로 갱신한다. 알고리즘 수행 전의 평활함수와 수행 후의 평활함수의 변동분이 일정 기준(t)보다 작아지면 평활함수가 수렴하는 것으로 간주하고 알고리즘을 중단한다. 


##### 2.3.1.3. $n \ll p$ 데이터에서 활용성

GAM은 변수 간의 비선형관계를 포착해서 모형에 반응할 수 있도록 회귀모형을 확장한 분석방법이다. 변수 간의 관계에 대해 별도의 모수를 정하지 않아도 알고리즘을 통해서 반응변수의 분포 타입에 맞는 최적해를 구해준다는 점에서 원리를 이해하고 나면 복잡한 설정이 필요하지 않다는 점도 장점이다. 뿐만 아니라 회귀모형의 최대 강점이 해석가능성을 어느 정도 확보하고 있기 때문에 적합 후에 해석이 불가능한 신경망 계열의 모형과 비교했을 때에 장점을 가진다고 할 수 있다.  

하지만 해석가능성을 확보하기 위해 합의 형태로 변수 간의 관계를 유지한 것 때문에 생기는 변수 간의 교호작용 누락은 다른 선형회귀모형과 마찬가지로 '선형성'을 남겨놓았기 때문에 불가피하게 생기는 한계라고 볼 수 있다.

$n \ll p$ 데이터에서 활용성 측면에서도 명과 암이 있다. 반응변수와 설명변수 간의 선형관계 전제에서 오는 많은 적용성의 한계를 극복할 수 있는 모형이라는 점에서는 다양한 변수의 복잡한 관계를 한 번에 포획할 수 있다는 장점이 있다. 하지만 $n \ll p$ 데이터에서 발생하기 쉬운 변수 간의 상호작용을 모형에 담아내지 못하는 점은 모형의 한계라고 할 수 있다.

따라서, GAM을 다변량 데이터에 적용하기 위해서는 분석 방법의 장점과 단점을 신중하게 고려해야 한다. GAM의 전제조건으로써 장점이자 단점의 원천이 되는 설명변수 간의 독립이라는 가정은 현실 데이터에서 성립하기 어렵다. 그럼에도 GAM을 활용함으로써 얻을 수 있는 단일 설명변수와 반응변수 사이의 유연한 관계 포착이 분석의 핵심 주제와 직결된 경우에는 GAM을 이용함으로써 얻을 수 있는 이득이 더 커질 수 있다. 


---

#### 2.3.2. 예제해설
---

### 2.4. 결론

$n \ll p$ 데이터 처리 문제에서 회귀모형이 가지는 한계를 회귀모형의 기본 전제를 최대한 유지하는 범위 내에서 극복할 수 있는 여러 분석 방법을 살펴봤다. 하지만 이들 분석 방법은 모두 고유한 장점과 단점을 가지고 있다. 이는 $n \ll p$ 데이터가 가지는 한계와 동시에 이론적 간결함(parsimony)을 위한 회귀모형의 전제가 가지는 한계에서 기인한다. 이 두 가지 문제를 어떻게 다루느냐에 따라 이를 극복하는 방향이 달라진다. 변수선택의 경우 데이터의 한계를 극복하기 위해 데이터를 삭제하는 접근법을, 정규화 선형회귀와 GAM은 회귀모형의 가정을 완화하는 접근법을 취하고 있다. 지금까지 살펴본 바와 같이 어떤 분석 방법도 완전하게 데이터의 특성을 포착하지 못하고 있는 것을 확인할 수 있었다. 하지만 선형회귀 - 일반화 선형회귀/정규화 선형회귀 - GAM으로 전개되는 회귀분석의 고급 방법론의 전개 과정은 인공신경망의 기본 원리를 구현하는 과정과 매우 흡사하다. 실제로 이론적 분석 틀로써 회귀분석이 가지는 설명가능성을 극단적인 수준까지 포기하고 예측에만 초점을 맞춘 분석 도구가 인경신경망이라고 이해한다면, 지금까지 학습한 내용은 인공지능의 기초 원리에 대한 이해로도 볼 수 있다.

다음 시간에는 데이터의 고유한 속성을 통해 다변량 데이터를 분석하기 위한 시도인 주성분분석과 그 연관문제를 통해 다변량 데이터를 다루는 또다른 차원의 접근법에 대해 살펴보겠다.